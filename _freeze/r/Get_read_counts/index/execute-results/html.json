{
  "hash": "d1de9ffee4196cc82e8bbfbddb019d0f",
  "result": {
    "markdown": "---\ntitle: \"Get read counts\"\nauthor: \"Karen Cristine Goncalves, Ph.D.\"\ndescription: \"4th R class - transcriptomics sessions\"\nfontsize: 1.25em\nlinkcolor: grey\ncallout-appearance: simple\ncategories:\n  - R advanced\n  - \"Read alignment\"\n  - Omics\n  - Transcriptomics\nformat: \n  revealjs:\n    transition: none\n    theme: dark\n    scrollable: true\ndate: \"16 February 2024\"\nincremental: false\necho: true\nwarning: false\neval: true\ndraft: false\n---\n\n\n## Data processing before R\n\nShort read sequencing transcriptomic data (in the form of fastq files, sometimes compressed as fastq.gz) pass through the some steps before we can proceed to analysis shown in this class. \n\nThe pre-processing is typically done in servers (unix/bash language) and includes:\n\n1. Quality assessment and filtering\n2. IN THE ABSENCE OF A REFERENCE: Transcriptome assembly\n3. Read alignment/mapping\n4. Sorting and indexing\n\n<p style=\"color: red\">**The things shown in this class are for alignments onto reference genomes.**</p>\n\n## Things to know \n\n- Keep track how many reads passed the quality filtering and how many were mapped.\n\n- In the case of a new assembly, you can follow steps on [TrinityRNASeq wiki](https://github.com/trinityrnaseq/trinityrnaseq/wiki/Trinity-Transcript-Quantification) for transcript quantification.\n\n- Is it possible to do all the processing in R? YES\n    - Is it worth it? ...\n    \n      >- What you will do in R is use it to connect to another program that will do the job. So you end up using more memory (computer gets slower, takes a lot of time).\n      >- Some programs are not available for windows, so you may install all the packages necessary just to learn that you cannot use them.\n\n  \n## Data processing before R - detail\n\n1. Quality assessment and filtering:\n    - [fastp](https://github.com/OpenGene/fastp) (last update: 2023); [fastqc](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) (last update: 2023) followed by [trimmomatic](https://github.com/usadellab/Trimmomatic) (last update: 2021)\n2. Creation of a reference transcriptome: \n    - [trinity](https://github.com/trinityrnaseq/trinityrnaseq) (last update: 2023); [SOAPdenovo-Trans](https://github.com/aquaskyline/SOAPdenovo-Trans) (last update: 2022); [Trans-ABySS](https://www.bcgsc.ca/resources/software/trans-abyss) (last update: 2018); [Oases](https://github.com/dzerbino/oases) (last update: 2015); [Velvet](https://github.com/dzerbino/velvet) (last update: 2014)\n3. Read alignment/mapping:\n    - Alignment - finds the exact origin of the reads and their differences compared to the reference (like blasting each read against a genome or transcriptome). It is slow - each alignment may take hours depending on the amount of data and size of the reference. These export SAM or BAM files, requiring indexing (step 4):\n        - [bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) (last update: 2024); [STAR](https://github.com/alexdobin/STAR) (last update: 2023); [HISAT2](https://daehwankimlab.github.io/hisat2/) (last update: 2020); [bwa](https://bio-bwa.sourceforge.net/) (last update: 2017); [TOPHAT2](https://ccb.jhu.edu/software/tophat/index.shtml) (last update: 2016)\n    - Pseudoalignment/Mapping - hyper fast - each alignment takes minutes. They only give the genomic/transcriptomic origin of each read. These export the read count tables directly:\n        - [Salmon](https://salmon.readthedocs.io/en/latest/) (last update: 2023); [Kallisto](https://pachterlab.github.io/kallisto/about) (last update: 2019)\n4. If the aligner provides the result as SAM or not sorted by genomic coordinates, use `samtools view` to sort and export into sorted BAM. Then use `samtools index` to get the `bai` files.\n    - In the end, you should have 2 files per sample/replicate: `sampleX_repY_Aligned.sortedByCoord.out.bam` and `sampleX_repY_Aligned.sortedByCoord.out.bam.bai`\n\n:::{.callout-info}\n\nNote that if you only want to perform differential expression analysis, GO term enrichment, etc, you do not need to perform read alignment, just mapping suffices.\n\n:::\n\n## Data processing before R - code example\n  \nThe code below executes the 2 commands of the program STAR: the creation of the index of the genome and the alignment.\n\nThe texts within `${}` are variable that should be created before hand. This `${r1/_R1.fastq}` means \"remove _R1.fastq\" from `${r1}` (like a `gsub`)\n\n```{.bash}\n# Note that the adapter sequences depend on the type of sequencer\nfastp -i ${raw_R1} -I ${raw_R2}\\\n -o ${r1} -O ${r2}\\\n --qualified_quality_phred 20\\\n --unqualified_percent_limit 30\\\n --adapter_sequence=TCGTCGGCAGCGTCAGATGTGTATAAGAGACAG\\\n --adapter_sequence_r2=GTCTCGTGGGCTCGGAGATGTGTATAAGAGACAG\\\n --cut_front --cut_front_window_size 3\\\n --cut_right --cut_right_window_size 4\\\n --cut_right_mean_quality 15\\\n --length_required 25\\\n --html $DIR/fastpReports/${r1/_R1.fastq}.html\\\n --thread 8 \n\n# Let's decide if we want to map or align\nMODE=\"align\"\n\nif [[ $MODE == \"align\" ]]; then\n  STAR\\\n   --runThreadN ${NCPUS}\\\n   --runMode genomeGenerate\\\n   --genomeDir ${genomeIdxDIR}\\\n   --genomeFastaFiles ${genomeFastaFiles}\\\n   --sjdbGTFfile ${genomeGTFFile}\\\n   --sjdbOverhang ${readLength}\n   \n  STAR --genomeDir ${indexDIR}/\\\n   --runThreadN ${NCPUS} \\\n   --readFilesIn ${DIR}/clean_reads/${r1} ${DIR}/clean_reads/${r2} \\\n   --outFileNamePrefix ${DIR}/alignments/${r1/_R1.fastq}_\\\n   --outSAMtype BAM SortedByCoordinate \\\n   --outSAMunmapped Within\n\nelse\n  kallisto index -i ${genomeIdxDIR} ${genomeFastaFiles}\n  kallisto quant -i ${genomeIdxDIR}\\\n   -o ${DIR}/alignments/${r1/_R1.fastq}\\\n    ${DIR}/clean_reads/${r1}\\\n    ${DIR}/clean_reads/${r2}\nfi\n\n```\n\n## Installing the packages\n\nIf you use the a reference genome for which the annotation is available in Ensembl, you can create your transcript (or 5'/3'-UTR) database while you are waiting for the alignment of your reads.\n\nFor this, we need the packages \"GenomicFeatures\", \"GenomicAlignments\", \"biomaRt\", \"tidyverse\".\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbioconductor_pkgs = c(\"GenomicFeatures\", \"GenomicAlignments\", \"biomaRt\", \"Rsamtools\")\ncran_pkgs = \"tidyverse\"\n\ndevtools::source_gist(\"https://gist.github.com/KarenGoncalves/0db105bceff4ff69547ee25460dda978\")\n\ninstall_from_dif_sources(\n    cran_packages = cran_pkgs,\n    bioconductor_packages = bioconductor_pkgs\n)\n```\n:::\n\n\n\n## Setting up the scene\n\nThere are several biomarts (databases) stored in different Ensembl pages.\n\n:::{.panel-tabset}\n# biomarts\n\n:::{.callout-not}\nThere is a bacteria Ensembl, but there is not a biomart for it\n:::\n\n- [Animals](https://ensembl.org)\n  - host: `\"https://ensembl.org\"`\n  - mart: `\"ENSEMBL_MART_ENSEMBL\"` (currently on version 111)\n- [Plants](https://plants.ensembl.org)\n  - host: `\"https://plants.ensembl.org\"`\n  - mart: `\"plants_mart\"`  (currently on version 58)\n- [Fungi](https://fungi.ensembl.org)\n  - host: `\"https://fungi.ensembl.org\"`\n  - mart: `\"fungi_mart\"`  (currently on version 58)\n- [Protists](https://protists.ensembl.org)\n  - host: `\"https://protists.ensembl.org\"`\n  - mart: `\"protists_mart\"` (currently on version 58)\n\nThere are other biomarts in the each Ensembl. You can check their names and versions with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhost = \"https://plants.ensembl.org\"\nlistMarts(host = host)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            biomart                      version\n1       plants_mart      Ensembl Plants Genes 58\n2 plants_variations Ensembl Plants Variations 58\n```\n:::\n:::\n\n\n# Selecting a biomart dataset\n\nNext thing you need is the name of the dataset. \n\nYou can check the ones available by running:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmart = useMart(host = host, biomart = \"plants_mart\")\nlistDatasets(mart) # this is a table, so we can filter it\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# If I am working with Arabidopsis thaliana, I can search for it with filter\nlistDatasets(mart) %>%\n  filter(grepl(\"Arabidopsis thaliana\", description))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n            dataset                         description version\n1 athaliana_eg_gene Arabidopsis thaliana genes (TAIR10)  TAIR10\n```\n:::\n\n```{.r .cell-code}\n# Now I know I have to use the dataset \"athaliana_eg_gene\"\n```\n:::\n\n\n# Creating the transcript database\n\nNow we have all the data we need, we set our variables: \n\n```{.r}\n# Edit the following based on what you need\nbiomart = \"plants_mart\"\ndataset = \"athaliana_eg_gene\"\nprefix = \"ensembl_\"\nhost = \"https://plants.ensembl.org\"\n\n# The result will be saved as a .RData object, so we do not recalculate this every time.\ndir = \"./\" # current directory\noutputPath = paste0(dir, \"Arabidopsis_TxDb.RData\")\n```\n\nThen we create a Transcript DataBase (txdb) from Biomart:\n\n```{.r}\n# Following takes a while (a few minutes), so it is better to calculate this once than export the result\nTxDb <- makeTxDbFromBiomart(biomart = biomart,\n                            dataset = dataset,\n                            id_prefix = prefix,\n                            host = host)\n\n# TxDb is a weird type of data, difficult to access, so we get the information on the transcripts with the function transcriptsBy\ntx <- transcriptsBy(TxDb,\"gene\")\n\n# Then we save the two databases into the Rdata object.\nsave(TxDb, tx, file = outputPath)\n\n```\n\n# Whole script\n\nIf you want to use the script we worked on here, just copy it below and change the section \"VARIABLES\" to fit your needs\n\n```{.r}\n###########################################\n################ VARIABLES ################\n###########################################\nspecies = \"Athaliana\"\nbiomart = \"plants_mart\"\ndataset = \"athaliana_eg_gene\"\nprefix = \"ensembl_\"\nhost = \"https://plants.ensembl.org\"\ndir = \"./output_tables/\" \noutputPath = paste0(dir, species, \"_TxDb.RData\")\n\n###########################################\n################ Packages #################\n###########################################\n\nbioconductor_pkgs = c(\"GenomicFeatures\", \"GenomicAlignments\", \"biomaRt\")\n\ndevtools::source_gist(\"https://gist.github.com/KarenGoncalves/0db105bceff4ff69547ee25460dda978\")\n\ninstall_from_dif_sources(\n    bioconductor_packages = bioconductor_pkgs\n)\n\n###########################################\n############# Create databse ##############\n###########################################\n\nTxDb <- makeTxDbFromBiomart(biomart = biomart,\n                            dataset = dataset,\n                            id_prefix = prefix,\n                            host = host)\ntx <- transcriptsBy(TxDb,\"gene\")\n\nsave(TxDb, tx, file = outputPath)\n```\n\n:::\n\n## Getting the input files\n\nNow that we have the `.bam`, `.bam.bai` and the transcript database RData, count the number of reads aligned to each gene.\n\nFor the example here, we will use these files (reads from _Arabidopsis thaliana_ Col-0 GFP plants):\n\n```{.r}\n\nsapply(c(\"output_tables/counts\", \"output_tables/alignments\"),\n       dir.create)\nsite = \"https://karengoncalves.github.io/Programming_classes/exampleData/\"\nsapply(c(\"subset_Col_0_GFP_1_Aligned.sortedByCoord.out.bam\",\n          \"subset_Col_0_GFP_1_Aligned.sortedByCoord.out.bam.bai\"),\n       \\(x) download.file(\n         url = paste0(site, x), \n         destfile = paste0(\"output_tables/alignments\", x))\n)    \n\n```\n\n\n## Get the counts - prepare the variables\n\nNow that we have the files, we need to inform R where they are stored and where to put the results, in other words, prepare the variables.\n\n:::{.callout-note}\n\n- The basename here is the name of the sample_rep, without the suffix added by the aligner or samtools nor folder names (these are removed with the function `basename`)\n- For the example, we will use a single sample, but we will use a loop so the final script can be used fo multiple samples easily\n\n:::\n\n```{.r}\nbamFiles <- paste0(\"/output_tables/alignments/\", \n                   list.files(path = \"/output_tables/alignments/\")\n)\n  \nbaiFiles <- paste0(bamFiles, \".bai\")\nbaseNames <- gsub(\"_Aligned.sortedByCoord.out.bam\", \"\",\n                  basename(bamFiles))\nTxDbPath <- \"./output_tables/Arabidopsis_TxDb.RData\"\nload(TxDbPath)\noutPutDir <- \"./output_tables/\"\n\n# Now we load the RData\n# This line tells R where the alignment files for the sample are and how much of them to read at a time\nbfl <- BamFileList(bamFiles, \n                   baiFiles, \n                   yieldSize=200000)\n```\n\n\n\n\n\n\n## Get the counts - overlaps\n\nSummarize overlaps\n\n:::{.panel-tabset}\n\n# Code \n\nThe code below uses `summarizedOverlaps` to count the number of alignments (`bfl` object) falling in each transcript (`tx` object, transcript database)\n\n\n::: {.cell}\n\n```{.r .cell-code}\noverlaps <- summarizeOverlaps(\n  features = tx, # genes' coordinates\n  reads = bfl, # bam and bai files\n  mode = \"Union\", \n  # mode can be also \"IntersectStrict\" or \"IntersectNotEmpty\"\n  singleEnd = F, fragments = T, \n  ignore.strand = T\n  )\n```\n:::\n\n\n# Explanation\n\nIn this case, we have paired-end reads (singleEnd = F, fragments = T), we ignore the strand information and we use the [union mode](https://htseq.readthedocs.io/en/release_0.11.1/count.html):\n\n![](https://htseq.readthedocs.io/en/release_0.11.1/_images/count_modes.png)\n\n\n:::{.callout-note}\nAll the bam/bai files are analysed here. So if you have many, R will take very long to run the code. You can increase or decrease the `yieldSize` in the function `BamFileList` (previous slide) to use more/less memory and increase/decrease the speed of the code.\n:::\n\n# Saving overlaps result\n\nWe save this result, so if there is an issue with the rest of the code, the heavy part of the program gets saved before R stops.\n\n```{.r}\n# \nsave(overlaps, file = paste0(outPutDir, \"overlaps.RData\"))\n```\n\n:::\n\n## Get counts - export table\n\nNow that we have the overlaps, we can get the counts.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Then we extract the counts slot, transform into a data.frame and export it as a csv file.\ncountAssays = assays(overlaps)$counts  %>% as.data.frame()\nnames(countAssays) <- baseNames\n\nwrite_csv(x = countAssays, \n          file = paste0(outPutDir, \"counts.csv\"), \n          col_names = T,\n          quote = \"none\")\n```\n:::\n\n\nWe can check the number of reads/fragments that passed the criteria to be considered mapped using `colSums`. By using this function, we get the result for all datasets included in the table, without needing to explicitly type any specific sample name.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncolSums(countAssays)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nsubset_Col_0_GFP_1 \n             51725 \n```\n:::\n:::\n\n\n\n## More tutorials and resources\n\n- ARTICLE: [a survey of best practices for RNA-seq data analysis](https://doi.org/10.1186/s13059-016-0881-8)\n- [Introduction to transcript quantification with Salmon](https://bioinformatics-core-shared-training.github.io/Bulk_RNAseq_Course_Nov23/Bulk_RNAseq_Course_Base/Markdowns/03_Quantification_with_Salmon_introduction.html)\n- [End-to-end analysis - RNAseq](https://www.alzheimersworkbench.ucsd.edu/EndToEndAnalysis_RNASeq.html)\n- [RNAseq analysis in R - University of Cambridge](https://bioinformatics-core-shared-training.github.io/RNAseq-R/)\n- [RNAseq-R](https://sbc.shef.ac.uk/RNAseq-R/)\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\r\n<script>\r\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\r\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\r\n  // slide changes (different for each slide format).\r\n  (function () {\r\n    // dispatch for htmlwidgets\r\n    function fireSlideEnter() {\r\n      const event = window.document.createEvent(\"Event\");\r\n      event.initEvent(\"slideenter\", true, true);\r\n      window.document.dispatchEvent(event);\r\n    }\r\n\r\n    function fireSlideChanged(previousSlide, currentSlide) {\r\n      fireSlideEnter();\r\n\r\n      // dispatch for shiny\r\n      if (window.jQuery) {\r\n        if (previousSlide) {\r\n          window.jQuery(previousSlide).trigger(\"hidden\");\r\n        }\r\n        if (currentSlide) {\r\n          window.jQuery(currentSlide).trigger(\"shown\");\r\n        }\r\n      }\r\n    }\r\n\r\n    // hookup for slidy\r\n    if (window.w3c_slidy) {\r\n      window.w3c_slidy.add_observer(function (slide_num) {\r\n        // slide_num starts at position 1\r\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\r\n      });\r\n    }\r\n\r\n  })();\r\n</script>\r\n\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}